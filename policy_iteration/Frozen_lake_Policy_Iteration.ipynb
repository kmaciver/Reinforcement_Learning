{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soo..\n",
    "\n",
    "The idea here is to play a little bit with the \"Frozen Lake\" classic gym enviroment.\n",
    "\n",
    "Here is a description from the open ai gym documentation \n",
    "\n",
    "https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\n",
    "\n",
    "\"\"\"\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "    If you step into one of those holes, you'll fall into the freezing water.\n",
    "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "    you navigate across the lake and retrieve the disc.\n",
    "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "        \n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "    \n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actions available are:\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create our enviroment with Frozen Lake\n",
    "env = gym.make(\"FrozenLake-v0\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before the enviroment takes into account a \"is_slippery\" state, this creates a transition probability where you can take an action at a particular state but there is a probability you will end up in a different state of what your action intended you to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define our initial states, states-values, discount factor, actions and threshold\n",
    "V = np.zeros(env.nS)\n",
    "S = np.arange(0,16)\n",
    "threshold = 1e-3\n",
    "gamma = 0.9\n",
    "actions = {'LEFT':0,'DOWN':1,'RIGHT':2,'UP':3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to solve this problem. In this notebook i will use the policy iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Lets first define a for policy evaluation'''\n",
    "def policy_evaluation(env, V, S, policy):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in S:\n",
    "            v_old = V[s]\n",
    "            expected_r = 0\n",
    "            expected_v = 0\n",
    "            transition = env.P[s][policy[s]]\n",
    "            for (probs, state_prime, r, done) in transition:\n",
    "                expected_r += probs * r\n",
    "                expected_v += probs * V[state_prime]\n",
    "            V[s] = expected_r + (gamma*expected_v)\n",
    "            delta = max(delta, abs(v_old - V[s]))\n",
    "        if delta <= threshold:\n",
    "            break\n",
    "    return(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Now lets create a function to generate a random policy and a function to generate policy improvements and iteration'''\n",
    "\n",
    "def random_policy(env, S, actions):\n",
    "    policy_random = {}\n",
    "    for s in S:\n",
    "        s_a = random.choice(list(actions.values()))\n",
    "        policy_random[s] = s_a\n",
    "    return(policy_random)\n",
    "\n",
    "\n",
    "def policy_improvement_iteration(env, V, S):\n",
    "    policy = random_policy(env, S, actions)\n",
    "    while True:\n",
    "        V = policy_evaluation(env, V, S, policy)\n",
    "        new_policy = dict()\n",
    "        for s in S:\n",
    "            best_a = None\n",
    "            best_p = float('-inf')\n",
    "            for a in actions.values():\n",
    "                expected_r = 0\n",
    "                expected_v = 0\n",
    "                transition = env.P[s][a]\n",
    "                for (probs, state_prime, r, done) in transition:\n",
    "                    expected_r += probs * r\n",
    "                    expected_v += probs * V[state_prime]\n",
    "                V_new = expected_r + (gamma*expected_v)\n",
    "                if V_new >= best_p:\n",
    "                    best_p = V_new\n",
    "                    best_a = a\n",
    "            new_policy[s] = best_a\n",
    "        if new_policy == policy:\n",
    "            break\n",
    "        else:\n",
    "            policy = new_policy\n",
    "    return(policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the algorithms and view the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--- 0.004044771194458008 seconds ---\n",
      "[[0 3 0 3]\n",
      " [0 3 2 3]\n",
      " [3 1 0 3]\n",
      " [3 2 1 3]]\n",
      "{'LEFT': 0, 'DOWN': 1, 'RIGHT': 2, 'UP': 3}\n"
     ]
    }
   ],
   "source": [
    "env.render()\n",
    "start_time = time.time()\n",
    "optimal_policy = policy_improvement_iteration(env, V, S)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "optimal_actions = np.asarray(list(optimal_policy.values()))\n",
    "print(optimal_actions.reshape(4,4), actions, sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, the actions at each state seem kind of counter-intuitive but thats because the \"is_slippery\" effect.\n",
    "\n",
    "Let's calculate the results again but turning off the \"is_slippery effect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "--- 0.0063838958740234375 seconds ---\n",
      "[[2 2 1 0]\n",
      " [1 3 1 3]\n",
      " [2 2 1 3]\n",
      " [3 2 2 3]]\n",
      "{'LEFT': 0, 'DOWN': 1, 'RIGHT': 2, 'UP': 3}\n"
     ]
    }
   ],
   "source": [
    "env2 = gym.make(\"FrozenLake-v0\", is_slippery=False)\n",
    "env2.render()\n",
    "start_time = time.time()\n",
    "optimal_policy = policy_improvement_iteration(env2, V, S)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "optimal_actions = np.asarray(list(optimal_policy.values()))\n",
    "print(optimal_actions.reshape(4,4), actions, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, without the \"is_slippery\" effect the calculated policy seems to be quite intuitive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
