{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soo..\n",
    "\n",
    "The idea here is to play a little bit with the \"Frozen Lake\" classic gym enviroment.\n",
    "\n",
    "Here is a description from the open ai gym documentation \n",
    "\n",
    "https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\n",
    "\n",
    "\"\"\"\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "    If you step into one of those holes, you'll fall into the freezing water.\n",
    "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "    you navigate across the lake and retrieve the disc.\n",
    "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "        \n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "    \n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actions available are:\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create our enviroment with Frozen Lake\n",
    "env = gym.make(\"FrozenLake-v0\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before the enviroment takes into account a \"is_slippery\" state, this creates a transition probability where you can take an action at a particular state but there is a probability you will end up in a different state of what your action intended you to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define our initial states, states-values, discount factor, actions and threshold\n",
    "V = np.zeros(env.nS)\n",
    "S = np.arange(0,16)\n",
    "threshold = 1e-3\n",
    "gamma = 0.9\n",
    "actions = {'LEFT':0,'DOWN':1,'RIGHT':2,'UP':3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to solve this problem. In this notebook i will use the value iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Lets first define a function that will use bellman optimality equation \n",
    "    to calculate the optimal state value functions for all states'''\n",
    "\n",
    "def optimal_state_value(env,S,V):\n",
    "    loop = True\n",
    "    i = 0\n",
    "    while loop == True:\n",
    "        delta = 0\n",
    "        for s in S:\n",
    "            v_old = V[s]\n",
    "            best_v = float('-inf')\n",
    "            for a in actions.values():\n",
    "                expected_v = 0\n",
    "                expected_r = 0\n",
    "                transitions = env.P[s][a]\n",
    "                for (probs, state_prime, r, done) in transitions:\n",
    "                    expected_r += probs * r\n",
    "                    expected_v += probs * V[state_prime] \n",
    "                v_new = expected_r + (gamma * expected_v)\n",
    "                if v_new > best_v:\n",
    "                    best_v = v_new\n",
    "            V[s] = best_v\n",
    "            delta = max(delta, abs(v_old - best_v))\n",
    "        if delta <= threshold:\n",
    "            loop = False\n",
    "        i+=1\n",
    "    return (V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Now lets use a policy imporvement function to calculate the optimal policy \n",
    "    given the optimal state values previously calculated'''\n",
    "\n",
    "def optimal_policy(env,S,V):\n",
    "    policy = np.zeros(env.nS)\n",
    "    V = optimal_state_value(env,S,V)\n",
    "    for s in S:\n",
    "        best_a = None\n",
    "        best_v = float('-inf')\n",
    "        for k,a in actions.items():\n",
    "            expected_v = 0\n",
    "            expected_r = 0\n",
    "            transitions = env.P[s][a]\n",
    "            for (probs, state_prime, r, done) in transitions:\n",
    "                expected_r += probs * r\n",
    "                expected_v += probs * V[state_prime] \n",
    "            v_new = expected_r + (gamma * expected_v)\n",
    "            if v_new > best_v:\n",
    "                best_v = v_new\n",
    "                best_a = a\n",
    "        policy[s] = best_a\n",
    "    return(policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the algorithms and view the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[[0. 3. 0. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "{'LEFT': 0, 'DOWN': 1, 'RIGHT': 2, 'UP': 3}\n"
     ]
    }
   ],
   "source": [
    "env.render()\n",
    "print(optimal_policy(env,S,V).reshape(4,4), actions, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, the actions at each state seem kind of counter-intuitive but thats because the \"is_slippery\" effect.\n",
    "\n",
    "Let's calculate the results again but turning off the \"is_slippery effect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[[1. 2. 1. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [2. 1. 1. 0.]\n",
      " [0. 2. 2. 0.]]\n",
      "{'LEFT': 0, 'DOWN': 1, 'RIGHT': 2, 'UP': 3}\n"
     ]
    }
   ],
   "source": [
    "env2 = gym.make(\"FrozenLake-v0\", is_slippery=False)\n",
    "env2.render()\n",
    "print(optimal_policy(env2,S,V).reshape(4,4), actions, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, without the \"is_slippery\" effect the calculated policy seems to be quite intuitive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
